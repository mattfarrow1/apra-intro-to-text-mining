---
title: "\0"
format: 
  revealjs:
    css: ["theme/theme.css"]
    scss: ["theme/custom.scss"]
    theme: simple
    preview-links: auto
    code-line-numbers: true
    code-annotations: hover
    width: 1280
    height: 720
    filters: 
      - filter.lua
    footer: "[Introduction to Text Mining](https://mattfarrow1.github.io/apra-intro-to-text-mining/)"
    title-slide-attributes:
      data-background-image: "/materials/slide templates/Slide1.png"
      data-background-size: contain
      data-background-opacity: "1"
      smaller: true
editor: visual
---

# Getting Started {background-image="/materials/slide templates/Slide3.png" background-size="contain"}

## Setup

`r fontawesome::fa("laptop-code", "black")` Open `text-mining.R` in RStudio to follow along.

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Setup`

## Setup

::: callout-tip
Before we get started, it's a good idea to Restart R. That way nothing we did before carries through this section. (Session ➔ Restart R)
:::

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

# Install packages
# install.packages("tidyverse")
# install.packages("tidytext")
# install.packages("wordcloud")
# install.packages("here")

# Load libraries
library(tidyverse)
library(tidytext)
library(wordcloud)
library(here)
```

```{r}
#| echo: false
library(tidyverse)
library(tidytext)
library(wordcloud)
library(here)
library(flipbookr)
```

## Load the data

```{r}
#| echo: true

# Load data
load(here::here("materials", "workshop_data.RData"))

df <- df |>
  mutate(Branch = case_match(Branch,
                             "Disneyland_California" ~ "California",
                             "Disneyland_HongKong" ~ "Hong Kong",
                             "Disneyland_Paris" ~ "Paris")) |>
  rename("Park" = Branch)
```

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Visualize the Data`

## Why Visualize?

-   Gives us a sense of the data
-   See the distribution of the data
-   Raise questions to explore
-   Help shape your process

## Create a basic histogram

```{r}
#| label: basic-histogram
#| echo: false
#| eval: false
#| output-location: column
#| code-line-numbers: true

ggplot(df, aes(x = Rating)) +
  geom_bar() +
  geom_bar(
    fill = "steelblue",
    color = "black"
  ) +
  labs(
    title = "Distribution of Ratings",
    x = "Rating",
    y = "Count"
  ) +
  theme_minimal()
```

`r flipbookr:::chunq_reveal("basic-histogram", title = "Create a basic histogram", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## Histogram by park

```{r}
#| label: park-histogram
#| echo: false
#| eval: false
#| output-location: column
#| code-line-numbers: true

ggplot(df, aes(Rating,
  fill = Park
)) +
  geom_bar() +
  geom_bar(color = "black") +
  labs(
    title = "Distribution of Ratings by Park",
    x = "Rating",
    y = "Count",
    fill = "Park"
  ) +
  scale_fill_brewer(palette = 3) +
  theme_bw() +
  theme(legend.position = "bottom")
```

`r flipbookr:::chunq_reveal("park-histogram", title = "Histogram by park", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## Reviews by year

What if we wanted to look at the park ratings over time? What would we need to do?

-   Make sure `Year_Month` is a date
-   Group by `Park`
-   Count ratings per year

## Reviews by year

```{r}
#| echo: true
#| output-location: column-fragment

ratings_by_year <- df |> 
  mutate(Year = year(Year_Month),
         Month = month(Year_Month)) |> 
  group_by(Park) |> 
  count(Year) |> 
  rename(Ratings = n)

ratings_by_year
```

## Reviews by year (an aside)

```{r}
#| label: pivot-wider
#| echo: false
#| eval: false
#| output-location: column-fragment

df |> 
  mutate(Year = year(Year_Month),
         Month = month(Year_Month)) |> 
  group_by(Park) |> 
  count(Year) |> 
  rename(Ratings = n) |> 
  pivot_wider(names_from = Park,
              values_from = Ratings) |> 
  arrange(desc(Year))
```

`r flipbookr:::chunq_reveal("pivot-wider", title = "Reviews by year (an aside)", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## Reviews by year

```{r}
#| label: reviews-by-year
#| echo: false
#| eval: false
#| output-location: column-fragment
#| code-line-numbers: true

ggplot(
  ratings_by_year,
  aes(Year,
    Ratings,
    color = Park
  )
) +
  geom_line() +
  labs(
    title = "Ratings by Park per Year",
    x = "Year",
    y = "Ratings"
  ) +
  scale_x_continuous(
    breaks = c(
      2010,
      2011,
      2012,
      2013,
      2014,
      2015,
      2016,
      2017,
      2018,
      2019,
      2020
    )
  ) +
  ggthemes::theme_economist()
```

`r flipbookr:::chunq_reveal("reviews-by-year", title = "Reviews by year", lcolw = "40", rcolw = "60", smallcode = TRUE)`

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Sample the Data`

## Look at one review

Brackets allow us to isolate a single cell.

```{r}
#| echo: true
#| output-location: column-fragment

df$Review_Text[11]
```

## Convert it to a `tibble`

```{r}
#| echo: true
#| output-location: column-fragment

sample <- tibble(
  line = 1,
  text = df$Review_Text[11]
)

sample
```

::: aside
[What is a tibble?](https://tibble.tidyverse.org/reference/tibble.html)
:::

## `unnest_tokens`

> "I went there with 2 daughters 1 y.o and 4..."

<br>

```{r}
#| echo: true
#| output-location: column-fragment

sample |>
  unnest_tokens(word, text) |> 
  head(n = 10)
```

## Advantages of `unnest_tokens`

-   splits text into one word/token per row along with the original line number
-   takes care of converting to lowercase
-   removes punctuation

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Process the Data`

## Number each review

```{r}
#| echo: true
#| output-location: column-fragment

reviews <- df |>
  group_by(Park) |>
  mutate(linenumber = row_number()) |>
  ungroup() |>
  select(Park,
    linenumber,
    text = Review_Text
  ) |>
  arrange(
    Park,
    linenumber
  )

head(reviews, n = 3)
```

## Unnest Tokens

```{r}
#| echo: true
#| output-location: column-fragment

# Unnest tokens
tidy_reviews <- reviews |> 
  unnest_tokens(word, text)

head(tidy_reviews)
```

## Stop Words

```{r}
#| echo: true
#| output-location: column-fragment

head(stop_words)
```

## `anti_join`

> An anti_join return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table.

![](https://psyteachr.github.io/glossary/images/joins/anti_join.png){fig-align="center" height="350"}

::: aside
[Source](https://psyteachr.github.io/ads-v1/joins.html#anti_join)
:::

## Remove stop words

```{r}
#| echo: true
#| output-location: column-fragment

# Unnest tokens and remove stop words
tidy_reviews <- tidy_reviews |>
  anti_join(stop_words)

head(tidy_reviews)
```

## Create Word Counts

```{r}
#| echo: true
#| output-location: column-fragment

# Perform word count
tidy_reviews |> 
  count(word, sort = TRUE) |> 
  head()
```

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Word Clouds`

## Create a word cloud

```{r}
#| echo: true
#| warning: false
#| message: false
#| output-location: slide

tidy_reviews |>
  count(word) |>
  with(wordcloud(word, 
                 n, 
                 max.words = 40,
                 colors=brewer.pal(8, "Dark2")))
```

::: aside
What jumps out to you? How might we improve it?
:::

## Custom stop words

-   Stop word libraries are great starting points

-   Organization- or industry-specific terms may not be helpful

-   Custom stop word lists can filter these out

<br>

`wc_removals <- c("day", "disney", "disneyland", "rides")`

## Word Cloud without Custom Stop Words

```{r}
#| echo: true
#| warning: false
#| message: false
#| output-location: slide

wc_removals <- c("day", "disney", "disneyland", "rides", "park")

tidy_reviews |>
  filter(!word %in% wc_removals) |>        # <1>
  count(word) |>
  with(wordcloud(word, 
                 n, 
                 max.words = 50,
                 colors=brewer.pal(8, "Dark2")))
```

1.  The `%in%` syntax allows you to look for a match inside a vector or column.

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `N-Grams`

## What are n-grams?

-   A continuous sequence of `n` words
-   Can be used 'as is', or run through a stemmer to get morphemes
-   Offers more context than single words

## Set up bigrams

```{r}
#| echo: true
#| output-location: column-fragment

# Unnest into bigrams
tidy_bigrams <- reviews |> 
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2) |> 
  filter(!is.na(bigram))

head(tidy_bigrams)
```

## Separate words

```{r}
#| echo: true
#| output-location: column-fragment

# Separate words
tidy_bigrams <- tidy_bigrams |>
  separate(bigram, 
           c("word1", 
             "word2"), 
           sep = " ")

head(tidy_bigrams)
```

## Remove stop words

```{r}
#| echo: true
#| output-location: column-fragment

# Remove stop words
tidy_bigrams <- tidy_bigrams |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

head(tidy_bigrams)
```

## Most frequent terms

```{r}
#| echo: true
#| output-location: column-fragment

tidy_bigrams |> 
  count(word1, word2, sort = TRUE)
```

## Reunite terms

```{r}
#| echo: true
#| output-location: column-fragment

# Reunite terms
tidy_bigrams <- tidy_bigrams |>
  unite(bigram, word1, word2, sep = " ")

head(tidy_bigrams)
```

## Look at top bigrams by park

```{r}
#| echo: true
#| output-location: column-fragment

tidy_bigrams |> 
  group_by(Park) |> 
  count(bigram) |> 
  arrange(desc(n)) |> 
  head()
```

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Word Frequencies`

## Most common words by park

```{r}
#| echo: true
#| output-location: column-fragment

review_words <- reviews |> 
  unnest_tokens(word, text) |> 
  count(Park, word, sort = TRUE)

head(review_words)
```

## Total words per park

```{r}
#| echo: true
#| output-location: column-fragment

total_words <- review_words |> 
  group_by(Park) |> 
  summarize(Total = sum(n))

total_words
```

## Join the data together

```{r}
#| echo: true
#| output-location: column-fragment

review_words <- left_join(review_words, 
                          total_words)

head(review_words)
```

## Calculate term frequency

```{r}
#| echo: true
#| output-location: column-fragment

freq_by_rank <- review_words |> 
  group_by(Park) |> 
  mutate(rank = row_number(),
         freq = n/Total) |> 
  ungroup()

kableExtra::kable(head(freq_by_rank))
```

## Calculate tf-idf

```{r}
#| echo: true
#| output-location: column-fragment

review_tf_idf <- review_words |> 
  bind_tf_idf(word, Park, n)

kableExtra::kable(head(review_tf_idf))
```

## Reviews with high tf-idf

```{r}
#| echo: true
#| output-location: column-fragment

review_tf_idf |> 
  select(-Total) |> 
  arrange(desc(tf_idf)) |> 
  head() |> 
  kableExtra::kable()
```

## Visualize high tf-idf words

```{r}
#| echo: true
#| output-location: slide

review_tf_idf |> 
  group_by(Park) |> 
  slice_max(tf_idf, n = 10) |> 
  ungroup() |> 
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = Park)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Park, ncol = 1, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

# Code Section {background-image="/materials/slide templates/Slide5.png" background-size="contain"}

➜ `Sentiment Analysis`

## Lexicons

-   AFINN
-   bing
-   loughran
-   nrc
-   SentiWordNet
-   WordNet

## AFINN

```{r}
#| echo: true
#| output-location: column-fragment
get_sentiments(lexicon = "afinn") |> 
  sample_n(10)
```

## AFINN example

```{r}
#| label: afinn-example
#| echo: false
#| eval: false
#| output-location: column-fragment
#| code-line-numbers: true

tidy_reviews |>
  group_by(Park, linenumber) |>
  inner_join(get_sentiments("afinn")) |>
  summarise(value = sum(value)) |> 
  ungroup() |> 
  ggplot(aes(linenumber, value, fill = Park)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  facet_wrap(~Park, ncol = 1, scales = "free_x")
```

`r flipbookr:::chunq_reveal("afinn-example", title = "AFINN example", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## bing

```{r}
#| echo: true
#| output-location: column-fragment

get_sentiments(lexicon = "bing") |> 
  sample_n(10)
```

## bing example

```{r}
#| label: bing-example
#| echo: false
#| eval: false
#| output-location: column-fragment
#| code-line-numbers: true

tidy_reviews |>
  group_by(Park) |>
  inner_join(get_sentiments("bing")) |>
  count(Park, index = linenumber, sentiment) |>
  ungroup() |>
  pivot_wider(
    names_from = sentiment,
    values_from = n,
    values_fill = 0
  ) |>
  mutate(sentiment = positive - negative) |>
  ggplot(aes(index, sentiment, fill = Park)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  facet_wrap(~Park, ncol = 1, scales = "free_x")
```

`r flipbookr:::chunq_reveal("bing-example", title = "bing example", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## nrc

```{r}
#| echo: true
#| output-location: column-fragment

get_sentiments(lexicon = "nrc") |> 
  sample_n(10)
```

## nrc example

```{r}
#| label: nrc-example
#| echo: false
#| eval: false
#| output-location: column-fragment
#| code-line-numbers: true

tidy_reviews |>
  right_join(get_sentiments("nrc"), 
             relationship = "many-to-many") |>
  filter(!is.na(sentiment)) |> 
  count(sentiment, sort = TRUE)
```

`r flipbookr:::chunq_reveal("nrc-example", title = "nrc example", lcolw = "40", rcolw = "60", smallcode = TRUE)`

## Questions?

![Source: giphy](https://i.giphy.com/media/ePsV4lBfsrPmo/giphy.webp){fig-align="center"}
