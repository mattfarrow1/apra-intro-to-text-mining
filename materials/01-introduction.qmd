---
title: "\0"
format: 
  revealjs:
    css: ["theme/theme.css"]
    theme: simple
    preview-links: auto
    filters: 
      - filter.lua
    footer: "[Introduction to Text Mining](https://mattfarrow1.github.io/apra-intro-to-text-mining/)"
    title-slide-attributes:
      data-background-image: "/materials/slide templates/Slide1.png"
      data-background-size: contain
      data-background-opacity: "1"
editor: visual
---

# Overall Goals

-   Understanding terminology
-   Keyword searches in Excel
-   Getting started with R
-   Text mining in R
-   Text mining in Python

```{r}
library(tidyverse)
library(tidytext)
library(wordcloud)
```

## Poll

1.  What do you think of when I say text mining?
2.  How much experience do you have with programming languages like SQL, R, or Python?
3.  What are you hoping to take away from this workshop?

# Terminology

## Text Mining

**Text mining** is the process of extracting quantitative data from qualitative information.

## Natural vs. Artificial Language

-   Natural languages are those that evolved or emerged gradually over time, largely unconsciously.

-   Artificial languages are those that were designed, crafted, or invented with conscious purpose, largely all at once and not gradually.

## Natural vs. Artificial Language

| Natural Language | Artificial Language |
|------------------|---------------------|
| English          | Java                |
| Chinese          | Python              |
| Spanish          | R                   |
| Greek            | Klingon             |
| Arabic           | Elvish              |

## Natural Language Processing (NLP)

The process of understanding what text is saying, and being able to analyze it grammatically.

## NLP Challenges

-   Large vocabulary

-   Complex syntax

-   Irregularities

-   Ambiguous semantics

-   Even more problems such as humor, irony, metaphor, connotation, neologisms

## Tokens & Tokenization

**Tokens** are meaningful units of text, most commonly individual words.

**Tokenization** is the process of breaking text down into tokens.

## Stop Words

**Stop words** are filler words that don't convey a lot of information (pronouns, articles, etc.)

*the, and, of, a, he, she, for, to, that*

## Normalization

**Normalization** involves standardizing text prior to analysis. It might include:

-   Expanding contractions

-   Removing stop words

-   Correcting misspellings

-   Stemming (if required)

## Morphemes

**Morphemes** are the units that our words are made of. *(Not the same as syllables)*

-   Cats = "Cat" + "s"

-   Running = "Run" + "ing"

-   Desirability = "Desire" + "able" + "ity"

## Morphemes

+---------------------------------------------------------------------+-------------------------------------------------------+
| Stems                                                               | Affixes                                               |
+=====================================================================+=======================================================+
| -   Supplies the main meaning                                       | -   Adds or alters meaning                            |
|                                                                     |                                                       |
| -   Can *sometimes* be a word in itself (e.g., "jump" in "jumping") | -   Cannot usually be a word by itself (e.g., "-ing") |
+---------------------------------------------------------------------+-------------------------------------------------------+

## Stems/Stemmer

-   **Stems**: the morpheme that supplies the main meaning of a word (*jump* in *jumping*)
-   **Stemmer**: a tool that takes a word and returns the stem (*running* -\> *run*)

## Term Frequency

**Term frequency** measures how often a term occurs in a document---in raw form, it is a simply a word count divided by the total number of words in the document.

$$
\frac{word\_count}{total\_words}
$$

## Document Frequency

**Document frequency** measures how common the term is within a domain represented by a corpus of documents (C*)*.

## Corpus

A **corpus** is a collection of documents that you want to perform NLP on.

## Inverse Document Frequency

**Inverse document frequency** is computed by dividing the total number of documents in the corpus by the number of documents containing our target term, and applying a log scale.

$$
idf(t)=1+log\frac{C}{1+df(t)}
$$

## Term Frequency-Inverse Document Frequency (TF-IDF)

Higher term frequency and a lower document frequency leads to a higher TF-IDF.

$$
tfidf=tf(idf)
$$

## Word and document similarity

-   **Word Similarity**: similarity between individual words or tokens
-   **Document Similarity**: similarity between short phrases or documents

## Jaccard Similarity

Calculates the similarity and diversity of a sample data set by looking at how many terms the two documents share, compared to the total vocabulary $$J(A, B)=\frac{|A \cap B|}{|A \cup B|}=\frac{|A \cap B|}{|A|+|B|-|A \cap B|}$$

## **Cosine Similarity**

Calculates the similarity between words $$\cos (\theta)=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}=\frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2 \cdot \sum_{i=1}^n B_i^2}}$$

## **Latent Semantic Analysis (LSA)**: 

An algorithm that reduces the dimensionality of the vector space in order to understand which words are most associated (i.e. king:male, queen:female, doctor:male, nurse:female)

## Levels of Analysis

-   **Lexical Analysis**: The most basic form of NLP, lexical analysis is focused on analyzing individual words.
-   **Syntactic Analysis**: concerned with processing the grammar of written words.
-   **Semantic Analysis**: builds on lexical and syntactic analyses in order to understand the meanings of words.
-   **Discourse Analysis**: Understanding inferences in language is the domain of discourse analysis.
